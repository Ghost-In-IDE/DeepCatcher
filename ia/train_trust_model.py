# Sistema de Entrenamiento ML para An√°lisis de Confiabilidad Facial
# Archivo: train_trust_model.py
# Ejecutar una sola vez para entrenar el modelo

import os
import cv2
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import face_recognition
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import requests
import zipfile
from tqdm import tqdm
import json

class TrustModelTrainer:
    def __init__(self, model_path="./trust_model.h5"):
        self.model_path = model_path
        self.dataset_path = "./training_dataset"
        self.model = None
        
        # Crear directorios
        os.makedirs(self.dataset_path, exist_ok=True)
        os.makedirs(f"{self.dataset_path}/trustworthy", exist_ok=True)
        os.makedirs(f"{self.dataset_path}/untrustworthy", exist_ok=True)
    
    def download_sample_dataset(self):
        """Descarga un dataset de muestra para entrenamiento"""
        print("üîÑ Descargando dataset de muestra...")
        
        # URLs de ejemplo (estos son datasets p√∫blicos reales)
        datasets = {
            "trustworthy": [
                "https://github.com/NVlabs/ffhq-dataset",  # Rostros de alta calidad
                "https://www.kaggle.com/datasets/jessicali9530/celeba-dataset"
            ],
            "untrustworthy": [
                "https://www.kaggle.com/datasets/dansbecker/5-celebrity-faces-dataset"
            ]
        }
        
        print("""
üìã INSTRUCCIONES PARA CONFIGURAR EL DATASET:

1. Descarga manualmente estos datasets:
   - CelebA: https://www.kaggle.com/datasets/jessicali9530/celeba-dataset
   - FFHQ: https://github.com/NVlabs/ffhq-dataset
   - UTKFace: https://susanqq.github.io/UTKFace/

2. Organiza las im√°genes:
   - Personas que consideres CONFIABLES ‚Üí ./training_dataset/trustworthy/
   - Personas que consideres NO CONFIABLES ‚Üí ./training_dataset/untrustworthy/

3. Recomendaci√≥n: M√≠nimo 500 im√°genes por categor√≠a

4. Una vez organizadas, ejecuta este script nuevamente.
        """)
        
        # Crear algunas im√°genes sint√©ticas para demo
        self.create_synthetic_samples()
    
    def create_synthetic_samples(self):
        """Crea muestras sint√©ticas para demostraci√≥n"""
        print("üé® Creando muestras sint√©ticas para demostraci√≥n...")
        
        # Generar rostros sint√©ticos b√°sicos para prueba
        for category in ["trustworthy", "untrustworthy"]:
            for i in range(50):  # 50 muestras por categor√≠a
                # Crear imagen sint√©tica (en un caso real, usar dataset real)
                img = np.random.randint(0, 255, (128, 128, 3), dtype=np.uint8)
                
                # Agregar un "rostro" b√°sico (rect√°ngulo)
                if category == "trustworthy":
                    cv2.rectangle(img, (30, 30), (98, 98), (100, 150, 200), -1)
                else:
                    cv2.rectangle(img, (30, 30), (98, 98), (50, 50, 150), -1)
                
                # Agregar caracter√≠sticas faciales b√°sicas
                cv2.circle(img, (50, 50), 5, (0, 0, 0), -1)  # Ojo izquierdo
                cv2.circle(img, (78, 50), 5, (0, 0, 0), -1)  # Ojo derecho
                cv2.circle(img, (64, 70), 3, (0, 0, 0), -1)  # Nariz
                cv2.ellipse(img, (64, 85), (10, 5), 0, 0, 180, (0, 0, 0), 2)  # Boca
                
                # Guardar imagen
                img_path = f"{self.dataset_path}/{category}/synthetic_{i:03d}.jpg"
                cv2.imwrite(img_path, img)
        
        print("‚úÖ Muestras sint√©ticas creadas (reemplaza con dataset real para mejores resultados)")
    
    def create_model(self):
        """Crea la arquitectura del modelo de confiabilidad"""
        print("üèóÔ∏è Creando arquitectura del modelo...")
        
        model = keras.Sequential([
            # Capas de preprocesamiento
            layers.Rescaling(1./255, input_shape=(128, 128, 3)),
            
            # Capas convolucionales - Extracci√≥n de caracter√≠sticas
            layers.Conv2D(32, (3, 3), activation='relu'),
            layers.BatchNormalization(),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            layers.Conv2D(64, (3, 3), activation='relu'),
            layers.BatchNormalization(),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            layers.Conv2D(128, (3, 3), activation='relu'),
            layers.BatchNormalization(),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            layers.Conv2D(256, (3, 3), activation='relu'),
            layers.BatchNormalization(),
            layers.MaxPooling2D((2, 2)),
            layers.Dropout(0.25),
            
            # Capas densas - Clasificaci√≥n
            layers.Flatten(),
            layers.Dense(512, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            
            layers.Dense(256, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            
            layers.Dense(128, activation='relu'),
            layers.Dropout(0.2),
            
            # Capa de salida
            layers.Dense(1, activation='sigmoid')  # 0=No confiable, 1=Confiable
        ])
        
        # Compilar modelo
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        self.model = model
        print("‚úÖ Modelo creado exitosamente")
        
        # Mostrar resumen
        model.summary()
        return model
    
    def preprocess_image(self, img_path):
        """Preprocesa una imagen para el entrenamiento"""
        try:
            # Cargar imagen
            img = cv2.imread(img_path)
            if img is None:
                return None
            
            # Convertir BGR a RGB
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
            # Detectar rostros
            face_locations = face_recognition.face_locations(img, model="hog")
            
            if not face_locations:
                # Si no detecta rostro, usar imagen completa redimensionada
                img = cv2.resize(img, (128, 128))
                return img
            
            # Usar el primer rostro detectado
            top, right, bottom, left = face_locations[0]
            face = img[top:bottom, left:right]
            
            # Redimensionar a tama√±o est√°ndar
            face = cv2.resize(face, (128, 128))
            
            return face
            
        except Exception as e:
            print(f"‚ùå Error procesando {img_path}: {e}")
            return None
    
    def load_dataset(self):
        """Carga y preprocesa el dataset completo"""
        print("üìÇ Cargando dataset...")
        
        X, y = [], []
        
        # Contar archivos totales
        total_files = 0
        for category in ["trustworthy", "untrustworthy"]:
            category_path = f"{self.dataset_path}/{category}"
            if os.path.exists(category_path):
                total_files += len([f for f in os.listdir(category_path) 
                                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))])
        
        if total_files == 0:
            print("‚ùå No se encontraron im√°genes en el dataset")
            return None, None, None, None
        
        print(f"üìä Procesando {total_files} im√°genes...")
        
        with tqdm(total=total_files, desc="Procesando im√°genes") as pbar:
            # Cargar im√°genes confiables (etiqueta = 1)
            trustworthy_path = f"{self.dataset_path}/trustworthy"
            if os.path.exists(trustworthy_path):
                for img_name in os.listdir(trustworthy_path):
                    if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):
                        img_path = os.path.join(trustworthy_path, img_name)
                        img = self.preprocess_image(img_path)
                        if img is not None:
                            X.append(img)
                            y.append(1)  # Confiable
                        pbar.update(1)
            
            # Cargar im√°genes no confiables (etiqueta = 0)
            untrustworthy_path = f"{self.dataset_path}/untrustworthy"
            if os.path.exists(untrustworthy_path):
                for img_name in os.listdir(untrustworthy_path):
                    if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):
                        img_path = os.path.join(untrustworthy_path, img_name)
                        img = self.preprocess_image(img_path)
                        if img is not None:
                            X.append(img)
                            y.append(0)  # No confiable
                        pbar.update(1)
        
        if not X:
            print("‚ùå No se pudieron procesar im√°genes v√°lidas")
            return None, None, None, None
        
        # Convertir a arrays numpy
        X = np.array(X, dtype=np.float32)
        y = np.array(y, dtype=np.float32)
        
        # Dividir en entrenamiento y prueba
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"‚úÖ Dataset cargado:")
        print(f"   - Entrenamiento: {len(X_train)} im√°genes")
        print(f"   - Prueba: {len(X_test)} im√°genes")
        print(f"   - Confiables: {np.sum(y == 1)} im√°genes")
        print(f"   - No confiables: {np.sum(y == 0)} im√°genes")
        
        return X_train, X_test, y_train, y_test
    
    def train_model(self, epochs=100, batch_size=32):
        """Entrena el modelo con el dataset"""
        print("üöÄ Iniciando entrenamiento del modelo...")
        
        # Crear modelo si no existe
        if self.model is None:
            self.create_model()
        
        # Cargar datos
        X_train, X_test, y_train, y_test = self.load_dataset()
        
        if X_train is None:
            print("‚ùå No se pudo cargar el dataset")
            return None
        
        # Callbacks para mejorar entrenamiento
        callbacks = [
            keras.callbacks.EarlyStopping(
                monitor='val_accuracy',
                patience=10,
                restore_best_weights=True
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=0.0001
            ),
            keras.callbacks.ModelCheckpoint(
                self.model_path,
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            )
        ]
        
        # Entrenar modelo
        print(f"üéØ Entrenando por m√°ximo {epochs} √©pocas...")
        history = self.model.fit(
            X_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(X_test, y_test),
            callbacks=callbacks,
            verbose=1
        )
        
        # Evaluaci√≥n final
        print("\nüìä Evaluaci√≥n final del modelo:")
        test_loss, test_accuracy, test_precision, test_recall = self.model.evaluate(
            X_test, y_test, verbose=0
        )
        
        print(f"‚úÖ Resultados finales:")
        print(f"   - Precisi√≥n: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
        print(f"   - Precisi√≥n (Precision): {test_precision:.4f}")
        print(f"   - Recall: {test_recall:.4f}")
        print(f"   - F1-Score: {2 * (test_precision * test_recall) / (test_precision + test_recall):.4f}")
        
        # Predicciones detalladas
        y_pred = (self.model.predict(X_test) > 0.5).astype(int)
        
        print("\nüìã Reporte de clasificaci√≥n:")
        print(classification_report(y_test, y_pred, 
                                  target_names=['No Confiable', 'Confiable']))
        
        print("\nüîÑ Matriz de confusi√≥n:")
        cm = confusion_matrix(y_test, y_pred)
        print(f"         Predicho")
        print(f"Real     No-Conf  Confiable")
        print(f"No-Conf    {cm[0,0]:4d}     {cm[0,1]:4d}")
        print(f"Confiable  {cm[1,0]:4d}     {cm[1,1]:4d}")
        
        # Guardar m√©tricas
        metrics = {
            "accuracy": float(test_accuracy),
            "precision": float(test_precision),
            "recall": float(test_recall),
            "f1_score": float(2 * (test_precision * test_recall) / (test_precision + test_recall)),
            "confusion_matrix": cm.tolist(),
            "training_samples": len(X_train),
            "test_samples": len(X_test)
        }
        
        with open("model_metrics.json", "w") as f:
            json.dump(metrics, f, indent=2)
        
        print(f"\nüíæ Modelo guardado en: {self.model_path}")
        print(f"üìä M√©tricas guardadas en: model_metrics.json")
        
        return history
    
    def test_model(self, test_image_path):
        """Prueba el modelo con una imagen espec√≠fica"""
        if not os.path.exists(self.model_path):
            print("‚ùå No se encontr√≥ modelo entrenado")
            return None
        
        # Cargar modelo
        model = keras.models.load_model(self.model_path)
        
        # Procesar imagen
        img = self.preprocess_image(test_image_path)
        if img is None:
            print("‚ùå No se pudo procesar la imagen de prueba")
            return None
        
        # Predecir
        img_batch = np.expand_dims(img, axis=0)
        prediction = model.predict(img_batch)[0][0]
        
        is_trustworthy = prediction > 0.5
        confidence = prediction if is_trustworthy else 1 - prediction
        
        print(f"\nüîç Resultado de prueba:")
        print(f"   - Imagen: {test_image_path}")
        print(f"   - Confiable: {'S√ç' if is_trustworthy else 'NO'}")
        print(f"   - Confianza: {confidence:.2%}")
        
        return {
            "trustworthy": is_trustworthy,
            "confidence": float(confidence),
            "raw_prediction": float(prediction)
        }

def main():
    print("ü§ñ SISTEMA DE ENTRENAMIENTO - AN√ÅLISIS DE CONFIABILIDAD FACIAL\n")
    
    trainer = TrustModelTrainer()
    
    print("Opciones disponibles:")
    print("1. Configurar dataset (descargar/organizar)")
    print("2. Entrenar modelo")
    print("3. Probar modelo con imagen")
    print("4. Proceso completo (configurar + entrenar)")
    
    choice = input("\nüëâ Selecciona una opci√≥n (1-4): ").strip()
    
    if choice == "1":
        trainer.download_sample_dataset()
        
    elif choice == "2":
        if not os.path.exists(f"{trainer.dataset_path}/trustworthy") or \
           not os.path.exists(f"{trainer.dataset_path}/untrustworthy"):
            print("‚ùå Dataset no configurado. Ejecuta opci√≥n 1 primero.")
            return
        
        epochs = input("N√∫mero de √©pocas (default: 100): ").strip()
        epochs = int(epochs) if epochs else 100
        
        trainer.train_model(epochs=epochs)
        
    elif choice == "3":
        img_path = input("Ruta de la imagen de prueba: ").strip()
        if os.path.exists(img_path):
            trainer.test_model(img_path)
        else:
            print("‚ùå Imagen no encontrada")
            
    elif choice == "4":
        print("üîÑ Ejecutando proceso completo...")
        trainer.download_sample_dataset()
        
        input("\n‚è∏Ô∏è Configura tu dataset manualmente y presiona ENTER para continuar...")
        
        trainer.train_model()
        
    else:
        print("‚ùå Opci√≥n no v√°lida")

if __name__ == "__main__":
    main()